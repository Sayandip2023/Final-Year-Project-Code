{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the dataset from the GCP Bucket"
      ],
      "metadata": {
        "id": "Vts6RijhfScK"
      },
      "id": "Vts6RijhfScK"
    },
    {
      "cell_type": "code",
      "id": "SaBh9hiMZpFQQhPTceyPDYld",
      "metadata": {
        "tags": [],
        "id": "SaBh9hiMZpFQQhPTceyPDYld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf64d582-908e-41c9-932e-971cc6bc26fb"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "\n",
        "# Download and extract dataset from GCS\n",
        "def download_and_extract_gcs(bucket_name, blob_path, destination_folder):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_path)\n",
        "\n",
        "    zip_path = os.path.join(destination_folder, \"dataset.zip\")\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "    blob.download_to_filename(zip_path)\n",
        "    print(f\"Downloaded {blob_path} to {zip_path}.\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_folder)\n",
        "    print(f\"Extracted dataset to {destination_folder}.\")\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Removed temporary zip file: {zip_path}.\")\n",
        "\n",
        "# Replace these with your actual GCS bucket name and blob path\n",
        "bucket_name = \"Data\"\n",
        "blob_path = \"eval/downloaded_file.zip\"\n",
        "destination_folder = \"./dataset\"\n",
        "\n",
        "# Download and extract the dataset\n",
        "download_and_extract_gcs(bucket_name, blob_path, destination_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded eval/downloaded_file.zip to ./dataset/dataset.zip.\n",
            "Extracted dataset to ./dataset.\n",
            "Removed temporary zip file: ./dataset/dataset.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "5IFV4-3efcWB"
      },
      "id": "5IFV4-3efcWB"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def inspect_dataset_structure(data_dir):\n",
        "    # List the classes (main folders such as microsleep, yawning)\n",
        "    classes = os.listdir(data_dir)\n",
        "    print(f\"Classes found in {data_dir}: {classes}\")\n",
        "\n",
        "    # Iterate through each class folder to check subfolders (videos or subdirectories)\n",
        "    for cls in classes:\n",
        "        class_dir = os.path.join(data_dir, cls)\n",
        "        if os.path.isdir(class_dir):\n",
        "            print(f\"\\nInspecting class folder: {cls}\")\n",
        "            subfolders = [f for f in os.listdir(class_dir) if os.path.isdir(os.path.join(class_dir, f))]\n",
        "\n",
        "            # If no subfolders, it means images might be directly inside the class folder\n",
        "            if not subfolders:\n",
        "                print(f\"  No subfolders found. Checking for image files directly in the {cls} folder...\")\n",
        "                image_files = [f for f in os.listdir(class_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "                if image_files:\n",
        "                    print(f\"  Found {len(image_files)} images directly in the {cls} folder.\")\n",
        "                else:\n",
        "                    print(f\"  No images found directly in {cls} folder.\")\n",
        "            else:\n",
        "                for subfolder in subfolders:\n",
        "                    subfolder_path = os.path.join(class_dir, subfolder)\n",
        "                    image_files = [f for f in os.listdir(subfolder_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "                    print(f\"  Found {len(image_files)} images in subfolder {subfolder_path}.\")\n",
        "\n",
        "# Path to your dataset folder (adjust if needed)\n",
        "data_dir = \"/content/dataset/Image data\"\n",
        "\n",
        "# Inspect the dataset structure\n",
        "inspect_dataset_structure(data_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7h-7TtFH99F",
        "outputId": "ea3242e4-7fc1-49bc-fbeb-4c6b49863f6f"
      },
      "id": "l7h-7TtFH99F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found in /content/dataset/Image data: ['test', 'Yawning', 'train', 'Microsleep', 'val']\n",
            "\n",
            "Inspecting class folder: test\n",
            "  No subfolders found. Checking for image files directly in the test folder...\n",
            "  No images found directly in test folder.\n",
            "\n",
            "Inspecting class folder: Yawning\n",
            "  Found 990 images in subfolder /content/dataset/Image data/Yawning/P1042773_720.\n",
            "  Found 1320 images in subfolder /content/dataset/Image data/Yawning/P1042748_720.\n",
            "  Found 865 images in subfolder /content/dataset/Image data/Yawning/P1042750_720.\n",
            "  Found 1000 images in subfolder /content/dataset/Image data/Yawning/P1042798_720.\n",
            "  Found 1490 images in subfolder /content/dataset/Image data/Yawning/P1042778_720.\n",
            "  Found 1027 images in subfolder /content/dataset/Image data/Yawning/P1042780_720.\n",
            "  Found 730 images in subfolder /content/dataset/Image data/Yawning/P1043062_720.\n",
            "  Found 815 images in subfolder /content/dataset/Image data/Yawning/P1042771_720.\n",
            "  Found 778 images in subfolder /content/dataset/Image data/Yawning/P1043060_720.\n",
            "  Found 1110 images in subfolder /content/dataset/Image data/Yawning/P1042752_720.\n",
            "  Found 490 images in subfolder /content/dataset/Image data/Yawning/P1042799_720.\n",
            "  Found 790 images in subfolder /content/dataset/Image data/Yawning/P1042749_720.\n",
            "  Found 660 images in subfolder /content/dataset/Image data/Yawning/P1042791_720.\n",
            "  Found 3118 images in subfolder /content/dataset/Image data/Yawning/P1042777_720.\n",
            "  Found 1510 images in subfolder /content/dataset/Image data/Yawning/P1042769_720.\n",
            "  Found 1280 images in subfolder /content/dataset/Image data/Yawning/P1042770_720.\n",
            "  Found 623 images in subfolder /content/dataset/Image data/Yawning/P1043061_720.\n",
            "  Found 595 images in subfolder /content/dataset/Image data/Yawning/P1042800_720.\n",
            "  Found 1090 images in subfolder /content/dataset/Image data/Yawning/P1042779_720.\n",
            "\n",
            "Inspecting class folder: train\n",
            "  No subfolders found. Checking for image files directly in the train folder...\n",
            "  No images found directly in train folder.\n",
            "\n",
            "Inspecting class folder: Microsleep\n",
            "  Found 3055 images in subfolder /content/dataset/Image data/Microsleep/P1043115_720.\n",
            "  Found 3145 images in subfolder /content/dataset/Image data/Microsleep/P1042797_720.\n",
            "  Found 3120 images in subfolder /content/dataset/Image data/Microsleep/P1043089_720.\n",
            "  Found 3095 images in subfolder /content/dataset/Image data/Microsleep/P1043129_720.\n",
            "  Found 3230 images in subfolder /content/dataset/Image data/Microsleep/P1042767_720.\n",
            "  Found 3285 images in subfolder /content/dataset/Image data/Microsleep/P1043067_720.\n",
            "  Found 3100 images in subfolder /content/dataset/Image data/Microsleep/P1042756_720.\n",
            "  Found 3100 images in subfolder /content/dataset/Image data/Microsleep/P1043106_720.\n",
            "  Found 3215 images in subfolder /content/dataset/Image data/Microsleep/P1042751_720.\n",
            "  Found 3125 images in subfolder /content/dataset/Image data/Microsleep/P1043122_720.\n",
            "  Found 3105 images in subfolder /content/dataset/Image data/Microsleep/P1042772_720.\n",
            "  Found 3288 images in subfolder /content/dataset/Image data/Microsleep/P1042787_720.\n",
            "  Found 3275 images in subfolder /content/dataset/Image data/Microsleep/P1042786_720.\n",
            "  Found 3095 images in subfolder /content/dataset/Image data/Microsleep/P1042762_720.\n",
            "  Found 3145 images in subfolder /content/dataset/Image data/Microsleep/P1042793_720.\n",
            "  Found 3665 images in subfolder /content/dataset/Image data/Microsleep/P1043068_720.\n",
            "  Found 3285 images in subfolder /content/dataset/Image data/Microsleep/P1043075_720.\n",
            "  Found 350 images in subfolder /content/dataset/Image data/Microsleep/P1043114_720.\n",
            "  Found 3650 images in subfolder /content/dataset/Image data/Microsleep/P1042757_720.\n",
            "\n",
            "Inspecting class folder: val\n",
            "  No subfolders found. Checking for image files directly in the val folder...\n",
            "  No images found directly in val folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Balancing"
      ],
      "metadata": {
        "id": "ZH8iMQn5fuJn"
      },
      "id": "ZH8iMQn5fuJn"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def balance_and_split_dataset(yawning_dir, microsleep_dir, output_dir, val_test_split=0.2):\n",
        "    # Create directories for balanced dataset\n",
        "    train_dir = os.path.join(output_dir, \"train\")\n",
        "    val_dir = os.path.join(output_dir, \"val\")\n",
        "    test_dir = os.path.join(output_dir, \"test\")\n",
        "\n",
        "    # Make sure the directories exist\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # List all subfolders in Yawning and Microsleep\n",
        "    yawning_subfolders = [os.path.join(yawning_dir, subfolder) for subfolder in os.listdir(yawning_dir) if os.path.isdir(os.path.join(yawning_dir, subfolder))]\n",
        "    microsleep_subfolders = [os.path.join(microsleep_dir, subfolder) for subfolder in os.listdir(microsleep_dir) if os.path.isdir(os.path.join(microsleep_dir, subfolder))]\n",
        "\n",
        "    # Collect all images from Yawning and Microsleep\n",
        "    yawning_images = [os.path.join(subfolder, img) for subfolder in yawning_subfolders for img in os.listdir(subfolder) if img.endswith('.jpg') or img.endswith('.png')]\n",
        "    microsleep_images = [os.path.join(subfolder, img) for subfolder in microsleep_subfolders for img in os.listdir(subfolder) if img.endswith('.jpg') or img.endswith('.png')]\n",
        "\n",
        "    # Balance the dataset (take as many from Microsleep as there are in Yawning)\n",
        "    min_samples = min(len(yawning_images), len(microsleep_images))\n",
        "    microsleep_images = random.sample(microsleep_images, min_samples)\n",
        "\n",
        "    # Combine the images\n",
        "    all_images = yawning_images + microsleep_images\n",
        "    all_labels = ['Yawning'] * len(yawning_images) + ['Microsleep'] * len(microsleep_images)\n",
        "\n",
        "    # Split the dataset into train, validation, and test\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(all_images, all_labels, test_size=val_test_split*2, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Move images to the appropriate directories\n",
        "    def move_images(image_paths, labels, target_dir):\n",
        "        for img_path, label in zip(image_paths, labels):\n",
        "            class_dir = os.path.join(target_dir, label)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "            shutil.copy(img_path, class_dir)\n",
        "\n",
        "    move_images(X_train, y_train, train_dir)\n",
        "    move_images(X_val, y_val, val_dir)\n",
        "    move_images(X_test, y_test, test_dir)\n",
        "\n",
        "    print(f\"Balanced and split dataset into {train_dir}, {val_dir}, and {test_dir}\")\n",
        "\n",
        "# Define your source directories and output directory\n",
        "yawning_dir = \"/content/dataset/Image data/Yawning\"\n",
        "microsleep_dir = \"/content/dataset/Image data/Microsleep\"\n",
        "output_dir = \"/content/dataset/balanced_data\"\n",
        "\n",
        "# Balance and split the dataset\n",
        "balance_and_split_dataset(yawning_dir, microsleep_dir, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rd8puDNRsRi",
        "outputId": "ea06afe1-53f1-4f54-a936-ba8776ae28db"
      },
      "id": "-Rd8puDNRsRi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced and split dataset into /content/dataset/balanced_data/train, /content/dataset/balanced_data/val, and /content/dataset/balanced_data/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "BFJZ-4Zkf2yx"
      },
      "id": "BFJZ-4Zkf2yx"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_data_with_augmentation(train_dir, val_dir, test_dir, img_size=(224, 224), batch_size=32):\n",
        "    # Set up data augmentation for training data\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1.0/255.0,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # For validation and test, only rescaling\n",
        "    val_test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "    # Load training, validation, and test data using the data generators\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory=train_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = val_test_datagen.flow_from_directory(\n",
        "        directory=val_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_generator = val_test_datagen.flow_from_directory(\n",
        "        directory=test_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator, test_generator\n",
        "\n",
        "# Set up directories for the balanced dataset\n",
        "train_dir = \"/content/dataset/balanced_data/train\"\n",
        "val_dir = \"/content/dataset/balanced_data/val\"\n",
        "test_dir = \"/content/dataset/balanced_data/test\"\n",
        "\n",
        "# Load the data\n",
        "train_gen, val_gen, test_gen = load_data_with_augmentation(train_dir, val_dir, test_dir)\n",
        "\n",
        "# Print class names to check\n",
        "print(\"Class Names:\", train_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbk3q6SBIZg6",
        "outputId": "c5f4f0de-3c58-40d3-92e0-33282a9373a5"
      },
      "id": "tbk3q6SBIZg6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5939 images belonging to 2 classes.\n",
            "Found 4674 images belonging to 2 classes.\n",
            "Found 4733 images belonging to 2 classes.\n",
            "Class Names: {'Microsleep': 0, 'Yawning': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "mj4te8FkgAGa"
      },
      "id": "mj4te8FkgAGa"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_model(input_shape=(224, 224, 3), num_classes=2):\n",
        "    model = Sequential([\n",
        "        # First convolutional block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Second convolutional block\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Third convolutional block\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        # Fourth convolutional block\n",
        "        Conv2D(256, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        # Global Average Pooling\n",
        "        GlobalAveragePooling2D(),\n",
        "\n",
        "        # Fully connected layer\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')  # Softmax activation for classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',  # Multi-class classification\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build and summarize the model\n",
        "model = build_model(input_shape=(224, 224, 3), num_classes=len(train_gen.class_indices))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "h2ocELxOIg5G",
        "outputId": "cee102f6-1e92-4d00-dcb4-13d892f644cf"
      },
      "id": "h2ocELxOIg5G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m514\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m456,642\u001b[0m (1.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">456,642</span> (1.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m455,682\u001b[0m (1.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">455,682</span> (1.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "HRZmD4QZgN-w"
      },
      "id": "HRZmD4QZgN-w"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "history = model.fit(\n",
        "    train_gen,  # Training data\n",
        "    validation_data=val_gen,  # Validation data\n",
        "    epochs=20,  # Number of epochs to train for\n",
        "    batch_size=32,  # Batch size for each iteration\n",
        "    verbose=1  # Print progress bar and details\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"balanced_model.h5\")\n",
        "print(\"Model saved as balanced_model.h5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C1z49eCIln5",
        "outputId": "ef124282-b1c4-4584-ba2b-c75acf8c95a7"
      },
      "id": "_C1z49eCIln5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 437ms/step - accuracy: 0.9055 - loss: 0.2497 - val_accuracy: 0.3579 - val_loss: 0.8749\n",
            "Epoch 2/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 366ms/step - accuracy: 0.9601 - loss: 0.1319 - val_accuracy: 0.3415 - val_loss: 1.6420\n",
            "Epoch 3/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 367ms/step - accuracy: 0.9666 - loss: 0.1021 - val_accuracy: 0.7037 - val_loss: 0.7583\n",
            "Epoch 4/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 365ms/step - accuracy: 0.9770 - loss: 0.0683 - val_accuracy: 0.8611 - val_loss: 0.6117\n",
            "Epoch 5/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 366ms/step - accuracy: 0.9811 - loss: 0.0522 - val_accuracy: 0.6635 - val_loss: 4.3192\n",
            "Epoch 6/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 365ms/step - accuracy: 0.9817 - loss: 0.0588 - val_accuracy: 0.9450 - val_loss: 0.1683\n",
            "Epoch 7/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 373ms/step - accuracy: 0.9840 - loss: 0.0451 - val_accuracy: 0.7865 - val_loss: 1.4107\n",
            "Epoch 8/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 365ms/step - accuracy: 0.9809 - loss: 0.0589 - val_accuracy: 0.6943 - val_loss: 7.4120\n",
            "Epoch 9/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 371ms/step - accuracy: 0.9836 - loss: 0.0494 - val_accuracy: 0.6846 - val_loss: 3.1558\n",
            "Epoch 10/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 364ms/step - accuracy: 0.9840 - loss: 0.0434 - val_accuracy: 0.8866 - val_loss: 0.6697\n",
            "Epoch 11/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 364ms/step - accuracy: 0.9918 - loss: 0.0234 - val_accuracy: 0.9191 - val_loss: 0.2904\n",
            "Epoch 12/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 364ms/step - accuracy: 0.9907 - loss: 0.0250 - val_accuracy: 0.7332 - val_loss: 3.7651\n",
            "Epoch 13/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 363ms/step - accuracy: 0.9874 - loss: 0.0391 - val_accuracy: 0.9442 - val_loss: 0.2459\n",
            "Epoch 14/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 361ms/step - accuracy: 0.9937 - loss: 0.0229 - val_accuracy: 0.9551 - val_loss: 0.1422\n",
            "Epoch 15/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 361ms/step - accuracy: 0.9884 - loss: 0.0312 - val_accuracy: 0.8299 - val_loss: 0.5492\n",
            "Epoch 16/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 362ms/step - accuracy: 0.9935 - loss: 0.0187 - val_accuracy: 0.9437 - val_loss: 0.1878\n",
            "Epoch 17/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 363ms/step - accuracy: 0.9941 - loss: 0.0146 - val_accuracy: 0.9431 - val_loss: 0.4332\n",
            "Epoch 18/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 360ms/step - accuracy: 0.9936 - loss: 0.0208 - val_accuracy: 0.9932 - val_loss: 0.0230\n",
            "Epoch 19/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 362ms/step - accuracy: 0.9910 - loss: 0.0269 - val_accuracy: 0.5601 - val_loss: 1.4680\n",
            "Epoch 20/20\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 362ms/step - accuracy: 0.9957 - loss: 0.0122 - val_accuracy: 0.9696 - val_loss: 0.0807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as balanced_model.h5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "1Ob-HFzwgTJ9"
      },
      "id": "1Ob-HFzwgTJ9"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the class names explicitly\n",
        "class_names = ['Microsleep', 'Yawning']\n",
        "\n",
        "# Evaluate the model on training data\n",
        "def evaluate_training_data(model, train_gen, class_names):\n",
        "    # Evaluate the model on the training data\n",
        "    train_loss, train_accuracy = model.evaluate(train_gen)\n",
        "    print(f\"Training Accuracy: {train_accuracy:.2%}\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Predictions and true labels for training data\n",
        "    y_pred_train = np.argmax(model.predict(train_gen), axis=1)\n",
        "    y_true_train = train_gen.classes\n",
        "\n",
        "    # Classification report for training data\n",
        "    train_report = classification_report(y_true_train, y_pred_train, target_names=class_names)\n",
        "    print(\"Training Classification Report:\\n\", train_report)\n",
        "\n",
        "    # Confusion matrix for training data\n",
        "    train_cm = confusion_matrix(y_true_train, y_pred_train)\n",
        "    print(\"Training Confusion Matrix:\\n\", train_cm)\n",
        "\n",
        "    # Misclassified examples (indices of wrong predictions)\n",
        "    misclassified_train_idx = np.where(y_true_train != y_pred_train)[0]\n",
        "    print(f\"Number of misclassified samples in training: {len(misclassified_train_idx)}\")\n",
        "\n",
        "    return train_report, train_cm, train_accuracy, train_loss, misclassified_train_idx\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "def evaluate_model(model, test_gen, class_names):\n",
        "    # Evaluate the model on the test data\n",
        "    test_loss, test_accuracy = model.evaluate(test_gen)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Predictions and true labels for test data\n",
        "    y_pred_test = np.argmax(model.predict(test_gen), axis=1)\n",
        "    y_true_test = test_gen.classes\n",
        "\n",
        "    # Classification report for test data\n",
        "    test_report = classification_report(y_true_test, y_pred_test, target_names=class_names)\n",
        "    print(\"Test Classification Report:\\n\", test_report)\n",
        "\n",
        "    # Confusion matrix for test data\n",
        "    test_cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "    print(\"Test Confusion Matrix:\\n\", test_cm)\n",
        "\n",
        "    # Misclassified examples (indices of wrong predictions)\n",
        "    misclassified_test_idx = np.where(y_true_test != y_pred_test)[0]\n",
        "    print(f\"Number of misclassified samples in test: {len(misclassified_test_idx)}\")\n",
        "\n",
        "    return test_report, test_cm, test_accuracy, test_loss, misclassified_test_idx\n",
        "\n",
        "# Call the evaluation functions\n",
        "train_report, train_cm, train_accuracy, train_loss, misclassified_train = evaluate_training_data(model, train_gen, class_names)\n",
        "test_report, test_cm, test_accuracy, test_loss, misclassified_test = evaluate_model(model, test_gen, class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHbppRqcYFi6",
        "outputId": "f62ed9d9-3032-4bac-e969-471ccc92d5e4"
      },
      "id": "oHbppRqcYFi6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 357ms/step - accuracy: 0.8431 - loss: 0.5065\n",
            "Training Accuracy: 84.53%\n",
            "Training Loss: 0.5230\n",
            "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 347ms/step\n",
            "Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Microsleep       0.60      0.46      0.52      3510\n",
            "     Yawning       0.42      0.56      0.48      2429\n",
            "\n",
            "    accuracy                           0.50      5939\n",
            "   macro avg       0.51      0.51      0.50      5939\n",
            "weighted avg       0.53      0.50      0.51      5939\n",
            "\n",
            "Training Confusion Matrix:\n",
            " [[1628 1882]\n",
            " [1070 1359]]\n",
            "Number of misclassified samples in training: 2952\n",
            "\u001b[1m  5/148\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.9845 - loss: 0.0666"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - accuracy: 0.9627 - loss: 0.0942\n",
            "Test Accuracy: 97.00%\n",
            "Test Loss: 0.0719\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "  Microsleep       0.99      0.96      0.98      3119\n",
            "     Yawning       0.93      0.98      0.96      1614\n",
            "\n",
            "    accuracy                           0.97      4733\n",
            "   macro avg       0.96      0.97      0.97      4733\n",
            "weighted avg       0.97      0.97      0.97      4733\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[3009  110]\n",
            " [  32 1582]]\n",
            "Number of misclassified samples in test: 142\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}